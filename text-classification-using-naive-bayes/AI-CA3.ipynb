{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI - CA3 - Text Classification using Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment, the purpose is to train a classifier model (based on Naive Bayes) to be able to classify news based on their short descriptions\n",
    "\n",
    "## The Classifier Model\n",
    "\n",
    "The classifier model uses the Bayes Theorem (the model is thoroughly explained below).\n",
    "The model uses the following formula: \n",
    "\n",
    "$$ P(c_i | X) = \\frac{P(X | c_i)P(c_i)}{P(X)}$$\n",
    "\n",
    "### The model definitions\n",
    "\n",
    "#### Posterior Probability\n",
    "The Posterior Probability is the probability of a news piece being in category $c_i$ if it include the words $X$\n",
    "\n",
    "#### Likelihood\n",
    "The probability of the word $x_i$ being in a news of category $c_i$. Calculating this value is explained later in the report (in _Training the Model_ part specifically)\n",
    "\n",
    "#### Class Prior Probability\n",
    "The probability of a news piece being in category $c_i$. This is calculated by dividing the number of news pieces in the $c_i$ category by the total number of news pieces.\n",
    "\n",
    "#### Predictor prior probability\n",
    "This is the probability of a word being $x_1,x_2,...\\ or\\  x_n$. This value is constant and can be obtained from the dataset, it will be ommited in the processing calculations for the sake of simplicity as it wont have any effects on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Classifier Works\n",
    "\n",
    "### Cleaning the given data\n",
    "A set of training data is given to the model. In this assignment we want to use the words of the _short_description_ text as features, so every other column from the dataset is ommitted in the explanations as well as the codes. \n",
    "\n",
    "At first, the short description texts are cleaned. Cleaning the texts consist of removing the stopwords and the punctuation marks, replacing uppercase characters with their lowercase counterparts (because while classifying the news, _Program_ and _program_ are the same to us and shouldn't be counted as different words)  and finally words are replaced by their roots using either **lemmatization** or **stemming** (These processes are explained later in the report). The words are reduced to their roots because _program_, _programmed_ and _programming_ are all the same to us while trying to classify the news, and with reducing them we have the sum of their repitions as the repition count of _program_, which results in it's probability being higher and it's effect more considerable.\n",
    "\n",
    "### Preparing the data\n",
    "Afterwards, the data are grouped by their category and splitted into two parts, the training data and the test data. In this report, 80% of the data is used for training and the remaining 20% is used for testing but these numbers can be easily changed. \n",
    "\n",
    "### Training the model\n",
    "In the next step, the classifier calculates $P(x_i | c_i)$ ($x$ being a word, and $c$ a category) for each word and each category. $P(x_i | c_i)$ is calculated using this formula: $\\frac{R_{x_i,c_i}}{W_{c_i}}$ where $R_{x_i,c_i}$ is the number of repetitions of $x_i$ in $c_i$ category news and $W_{c_i}$ is the total word count of the news pieces in category $c_i$.\n",
    "\n",
    "### Predicting the categories\n",
    "Finally, the classifier uses the previously calculated $P(x_i | c_i)$ values to calculate $P(c_i | X)\\ (X = x_1,x_2,...,x_n)$.\n",
    "To calculate this, we use Bayes' theorem: \n",
    "\n",
    "$$ P(c_i | X) = \\frac{P(X | c_i)P(c_i)}{P(X)} = \\frac{P(x_1,x_2,...,x_n | c_i)P(c_i)}{P(X)}$$\n",
    "\n",
    "As we are using Naive Bayes in this assignment, each feature is assumed to be independent from the rest. So the above formula will be as following:\n",
    "\n",
    "$$\\frac{P(x_1,x_2,...,x_n | c_i)P(c_i)}{P(X)} = \\frac{P(x_1 | c_i)P(x_2 | c_i)P(x_3 | c_i)...P(x_n | c_i)P(c_i)}{P(X)}$$\n",
    "\n",
    "As everything is being divided by $P(X)$ and $P(X)$ is a constant, it can be ommitted for the calculations to be simpler:\n",
    "\n",
    "$${P(x_1,x_2,...,x_n | c_i)P(c_i)} = {P(x_1 | c_i)P(x_2 | c_i)P(x_3 | c_i)...P(x_n | c_i)P(c_i)}$$\n",
    "\n",
    "To prevent the numbers becoming too small and resulting in a loss of accuracy, we use logarithm:\n",
    "\n",
    "$$\\log_{10}P(c_i | X) = \\log_{10}({P(x_1 | c_i)P(x_2 | c_i)P(x_3 | c_i)...P(x_n | c_i)P(c_i)})\\\\ = \\log_{10}P(x_1 | c_i) + \\log_{10}P(x_2 | c_i) + ... + \\log_{10}P(x_n | c_i) + \\log_{10}P(c_i)$$\n",
    "\n",
    "To predict the news category, $P(c_i | X)$ is calculated for each category, and the one with the maximum probability is regarded as the predicted category.\n",
    "\n",
    "#### Absent words in a category\n",
    "If a word is absent in a category, that is it hasn't been seen in the data pieces of this category, it's repetition count will be zero, and thus, it's $P(x | c_i)$ will be zero too which can result in the whole $P(X|c_i)$ becoming zero which is not desirable at all. In order to solve this problem and preventing the probability from becoming zero, the repetition count of absent words will be set to 1 as it's the lowest logical count.\n",
    "\n",
    "### Confusion Matrix\n",
    "The confusion matrix is a matrix used to visualize the performance of a model. In the confusion matrix A, the cell $A_{i,j}$ shows the number of times the predicted class was $j$, while the actual class was $i$, thus making the cells in the form $A_{i,i}$ the number of correct predictions per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the words to their roots\n",
    "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set. \n",
    "\n",
    "### Lemmatization\n",
    "**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "### Stemming\n",
    "**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n",
    "\n",
    "**The classifier designed in this assignment can use either of these methods.**\n",
    "\n",
    "As seen further below, the stemming and lemmatization methods get very similar results in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the TD-IDF weight?\n",
    "TF-IDF is a weight used to find the importance of a word to data-mining or classifying projects. \n",
    "\n",
    "TF stands for Term Frequency, it measures how frequently a word occurs in a document. as this value is heavily dependent on the document length, it's usually divided by the length of the document.\n",
    "\n",
    "IDF stands for Inverse Document Frequency, it measures the importance of a word by counting the number of documents this word occurs in. Words like the stopwords occur repetitively in many documents but are not of high importance, so the IDF measure tries to weigh down the words with a low level of importance but the rare words are weighed up.\n",
    "\n",
    "A simple implementation of the TF measure would be:\n",
    "\n",
    "$$TF(t) = \\frac{Number\\ of\\ times\\ term\\ t\\ appears\\ in\\ a\\ document}{Total\\ number\\ of\\ terms\\ in\\ the\\ document}$$\n",
    "\n",
    "and the implementation of IDF:\n",
    "\n",
    "$$IDF(t) = log_e(\\frac{Total\\ number\\ of\\ documents}{Number\\ of\\ documents\\ with\\ term\\ t\\ in\\ it})$$\n",
    "\n",
    "In this assignment, TF is used (although without dividing the word count by the document length) but if IDF is to be included, we could divide the $P(x_i|c_i)$ by the number of categories which include $x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:26.121132Z",
     "start_time": "2020-05-02T12:44:25.698308Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import prettytable as pt\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "DATASET_FILENAME = './data.csv'\n",
    "TEST_FILENAME = './test.csv'\n",
    "OUTPUT_FILENAME = './output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:26.127313Z",
     "start_time": "2020-05-02T12:44:26.122728Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_results_table(results):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Category', 'Normalization Method', 'Oversampling', 'Recall', 'Precision', 'Accuracy']\n",
    "    rows = []\n",
    "    for category, category_results in results['categories'].items():\n",
    "        rows.append([category, results['normalization_method'], results['oversampling'], category_results['recall'], category_results['precision'], results['accuracy']])\n",
    "    for row in rows:\n",
    "        table.add_row(row)\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n",
    "As seen below, the dataset contains some rows which have empty short_description columns. These rows are deleted in the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:26.247620Z",
     "start_time": "2020-05-02T12:44:26.129869Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "\n",
       "                                   short_description  \n",
       "0  Påskekrim is merely the tip of the proverbial ...  \n",
       "1                                                NaN  \n",
       "2  Madonna is slinking her way into footwear now,...  \n",
       "3  But what if you're a 30-something couple that ...  \n",
       "4  Obamacare was supposed to make birth control f...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(DATASET_FILENAME)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:26.261419Z",
     "start_time": "2020-05-02T12:44:26.249381Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22925 entries, 0 to 22924\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   index              22925 non-null  int64 \n",
      " 1   authors            18523 non-null  object\n",
      " 2   category           22925 non-null  object\n",
      " 3   date               22925 non-null  object\n",
      " 4   headline           22924 non-null  object\n",
      " 5   link               22925 non-null  object\n",
      " 6   short_description  21703 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:26.296524Z",
     "start_time": "2020-05-02T12:44:26.263417Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \n",
    "    def __init__(self, normalization_method):\n",
    "        if normalization_method == 'lemmatization':\n",
    "            self.normalize = self.normalize_with_lemmatization\n",
    "        elif normalization_method == 'stemming':\n",
    "            self.normalize = self.normalize_with_stemmization\n",
    "        else:\n",
    "            raise ValueError(\"normalization_method value should be either 'lemmatization' or 'stemming'\")\n",
    "    \n",
    "    def clean(self, dataset, desired_categories=None):\n",
    "        dataset = dataset[dataset.short_description.notnull()]\n",
    "        dataset = dataset.filter(['index', 'category', 'short_description'])\n",
    "        dataset['short_description'] = dataset['short_description'].apply(self.normalize)\n",
    "        return dataset[dataset.category.isin(desired_categories)] if desired_categories else dataset\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_with_lemmatization(text):\n",
    "        def get_mapped_tag(nltk_tag):\n",
    "            if nltk_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif nltk_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif nltk_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif nltk_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        sentences = sent_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatized_words = set()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for sentence in sentences:\n",
    "            tokens = [token for token in word_tokenize(sentence) if (token not in stop_words and token.isalnum())]\n",
    "            nltk_tagged = pos_tag(tokens)\n",
    "            wn_tagged = map(lambda x: (x[0], get_mapped_tag(x[1])), nltk_tagged)\n",
    "            for word, tag in wn_tagged:\n",
    "                if tag is None:                        \n",
    "                    lemmatized_words.add(word)\n",
    "                else:\n",
    "                    lemmatized_words.add(lemmatizer.lemmatize(word, tag))\n",
    "        return ','.join([word.lower() for word in lemmatized_words])\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_with_stemmization(text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [token for token in word_tokenize(text) if (token not in stop_words and token.isalnum())]\n",
    "        tokens = map(stemmer.stem, tokens)\n",
    "        return ','.join([token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:26.437998Z",
     "start_time": "2020-05-02T12:44:26.298481Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    ds = None\n",
    "    test_data = None\n",
    "    train_data = None\n",
    "    category_word_count = None\n",
    "    category_total_word_count = None\n",
    "    word_probabilities = None\n",
    "    total_dataset_length = None\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset, \n",
    "        normalization_method, \n",
    "        desired_categories, \n",
    "        test_data_filename, \n",
    "        training_data_percentage,\n",
    "        output_filename,\n",
    "        oversample=False\n",
    "    ):\n",
    "        self.data_cleaner = DataCleaner(normalization_method)\n",
    "        dataset = self.data_cleaner.clean(dataset, desired_categories)\n",
    "        self.categories = set(dataset['category'])\n",
    "        self.ds = {}\n",
    "        self.total_dataset_length = len(dataset)\n",
    "        for category in self.categories:\n",
    "            self.ds[category] = dataset[dataset['category'] == category].reset_index(drop=True)\n",
    "        self.training_data_percentage = training_data_percentage\n",
    "        self._prepare_train_and_test_data(oversample)\n",
    "        self._train()\n",
    "        self.test_data_filename = test_data_filename\n",
    "        self.output_filename = output_filename\n",
    "        self.is_oversampled = oversample\n",
    "        self.normalization_method = normalization_method\n",
    "        \n",
    "\n",
    "    def _prepare_train_and_test_data(self, oversample):\n",
    "        self.test_data = {}\n",
    "        self.train_data = {}\n",
    "        for category, data in self.ds.items():\n",
    "            partition_point = len(data) * self.training_data_percentage // 100\n",
    "            self.train_data[category] = data[:partition_point]\n",
    "            self.test_data[category] = data[partition_point:]\n",
    "        if oversample:\n",
    "            self.oversample()\n",
    "\n",
    "    def oversample(self):\n",
    "        max_category_data_count = len(max(self.train_data.values(), key=len))\n",
    "        for category, data in self.train_data.items():\n",
    "            lst = [data]\n",
    "            lst.append(data.sample(\n",
    "                max_category_data_count-len(data), replace=True))\n",
    "            self.train_data[category] = pd.concat(lst)\n",
    "\n",
    "    def _find_word_count(self):\n",
    "        self.category_word_count = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.category_total_word_count = defaultdict(lambda: 0)\n",
    "        for category, data in self.train_data.items():\n",
    "            for text in data['short_description']:\n",
    "                tokens = text.split(',')\n",
    "                for token in tokens:\n",
    "                    self.category_word_count[category][token] += 1\n",
    "                self.category_total_word_count[category] += len(tokens)\n",
    "\n",
    "    def _train(self):\n",
    "        self._find_word_count()\n",
    "        self.word_probabilities = defaultdict(dict)\n",
    "        for category, word_count in self.category_word_count.items():\n",
    "            for word, count in word_count.items():\n",
    "                self.word_probabilities[category][word] = math.log10(\n",
    "                    count / self.category_total_word_count[category])\n",
    "\n",
    "    def _predict_news_category(self, text):\n",
    "        results = {}\n",
    "        tokens = text.split(',')\n",
    "        for category in self.categories:\n",
    "            probability = math.log10(\n",
    "                len(self.ds[category]) / self.total_dataset_length)\n",
    "            for token in tokens:\n",
    "                probability += self.word_probabilities[category].get(\n",
    "                    token, math.log10(1/self.category_total_word_count[category]))\n",
    "            results[category] = probability\n",
    "        return max(results, key=results.get)\n",
    "\n",
    "    def check_with_test_data(self):\n",
    "        results = defaultdict(lambda: {'correct': 0, 'wrong': 0})\n",
    "        prediction_count = defaultdict(lambda: 0)\n",
    "        total_correct_predictions = 0\n",
    "        test_data_count = 0\n",
    "        actual_categories = []\n",
    "        predicted_categories = []\n",
    "        for category, data in self.test_data.items():\n",
    "            test_data_count += len(data)\n",
    "            actual_categories.extend([category] * len(data))\n",
    "            for index, row in data.iterrows():\n",
    "                predicted_category = self._predict_news_category(\n",
    "                    row['short_description'])\n",
    "                predicted_categories.append(predicted_category)\n",
    "                prediction_count[predicted_category] += 1\n",
    "                if predicted_category == category:\n",
    "                    results[category]['correct'] += 1\n",
    "                    total_correct_predictions += 1\n",
    "                else:\n",
    "                    results[category]['wrong'] += 1\n",
    "        confusion_matrix = ConfusionMatrix(actual_categories, predicted_categories)\n",
    "        category_results = defaultdict(dict)\n",
    "        accuracy = total_correct_predictions / test_data_count\n",
    "        for category in self.categories:\n",
    "            category_results[category]['recall'] = results[category]['correct'] / \\\n",
    "                len(self.test_data[category])\n",
    "            category_results[category]['precision'] = results[category]['correct'] / \\\n",
    "                prediction_count[category]\n",
    "        \n",
    "        return {\n",
    "            'categories': {**category_results},\n",
    "            'accuracy': accuracy,\n",
    "            'oversampling': self.is_oversampled,\n",
    "            'normalization_method': self.normalization_method,\n",
    "            'confusion_matrix': confusion_matrix\n",
    "        }\n",
    "\n",
    "    def predict(self):\n",
    "        data = pd.read_csv(self.test_data_filename)\n",
    "        data = self.data_cleaner.clean(data)\n",
    "        results = []\n",
    "        for index, row in data.iterrows():\n",
    "            results.append([row['index'], self._predict_news_category(row['short_description'])])\n",
    "        final_df = pd.DataFrame(results, columns=['index', 'category'])\n",
    "        final_df.to_csv(self.output_filename)\n",
    "        return final_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A problem with the dataset\n",
    "If we group the dataset by category, it can be seen that the data is imbalanced and the number of rows with BUSINESS category is much lower than the other two categories. To solve this problem, oversampling is used. In oversampling, random rows from the class with lower repetition are duplicated and appended to the dataset until all classes have an  almost identical number of data pieces and their Prior Class Probability becomes almost equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:26.456111Z",
     "start_time": "2020-05-02T12:44:26.439349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUSINESS 4568\n",
      "STYLE & BEAUTY 8674\n",
      "TRAVEL 8461\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = dataset[dataset.short_description.notnull()]\n",
    "sample_dataset = sample_dataset.groupby('category')\n",
    "for group, data in sample_dataset:\n",
    "    print(group, len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the functionalities\n",
    "A sample classifier is instantiated below to show the changes on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:40.154353Z",
     "start_time": "2020-05-02T12:44:26.457490Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method='stemming', \n",
    "        desired_categories=['TRAVEL', 'BUSINESS'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80,\n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "The training data are grouped by their categories, and if the oversampling argument value is set to True, the training data will be oversampled and each category will have an almost equal number of data pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:40.166262Z",
     "start_time": "2020-05-02T12:44:40.156150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BUSINESS':       index  category                                  short_description\n",
       " 0         4  BUSINESS  obamacar,suppos,make,birth,control,free,women,...\n",
       " 1        15  BUSINESS  in,fact,busi,insid,point,sunday,break,bad,audi...\n",
       " 2        18  BUSINESS  the,measur,part,effort,block,teenag,adolesc,ge...\n",
       " 3        23  BUSINESS  a,relief,well,compani,began,drill,earli,decemb...\n",
       " 4        26  BUSINESS  some,talk,american,banker,said,polit,pressur,a...\n",
       " ...     ...       ...                                                ...\n",
       " 177     942  BUSINESS                                  watch,wall,street\n",
       " 241    1268  BUSINESS  perhap,surprisingli,three,ivi,leagu,school,lis...\n",
       " 3058  15338  BUSINESS  when,think,great,leader,come,mind,richard,bran...\n",
       " 2627  13132  BUSINESS                  how,prepar,how,anybodi,prepar,you\n",
       " 2102  10559  BUSINESS  sinc,septemb,intergener,month,take,time,apprec...\n",
       " \n",
       " [6768 rows x 3 columns],\n",
       " 'TRAVEL':       index category                                  short_description\n",
       " 0         0   TRAVEL  påskekrim,mere,tip,proverbi,iceberg,oslo,citi,...\n",
       " 1         3   TRAVEL  but,coupl,shi,away,tabl,danc,encor,xs,what,hom...\n",
       " 2         9   TRAVEL  there,custom,confisc,tri,get,past,border,healt...\n",
       " 3        11   TRAVEL  the,trip,easi,escap,urban,everyday,life,none,s...\n",
       " 4        12   TRAVEL                  it,closest,thing,sleep,north,wall\n",
       " ...     ...      ...                                                ...\n",
       " 6763  18242   TRAVEL  i,find,i,feel,like,i,got,someth,captur,free,tr...\n",
       " 6764  18243   TRAVEL  it,difficult,measur,posit,come,one,visit,natio...\n",
       " 6765  18246   TRAVEL  later,histori,tintern,abbey,becam,frequent,des...\n",
       " 6766  18247   TRAVEL  the,surf,alway,somewher,whether,sandi,littl,ca...\n",
       " 6767  18248   TRAVEL  hot,coattail,discoveri,prettiest,beach,world,r...\n",
       " \n",
       " [6768 rows x 3 columns]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_classifier.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:40.178203Z",
     "start_time": "2020-05-02T12:44:40.167642Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>obamacar,suppos,make,birth,control,free,women,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>in,fact,busi,insid,point,sunday,break,bad,audi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>the,measur,part,effort,block,teenag,adolesc,ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>a,relief,well,compani,began,drill,earli,decemb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>some,talk,american,banker,said,polit,pressur,a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  category                                  short_description\n",
       "0      4  BUSINESS  obamacar,suppos,make,birth,control,free,women,...\n",
       "1     15  BUSINESS  in,fact,busi,insid,point,sunday,break,bad,audi...\n",
       "2     18  BUSINESS  the,measur,part,effort,block,teenag,adolesc,ge...\n",
       "3     23  BUSINESS  a,relief,well,compani,began,drill,earli,decemb...\n",
       "4     26  BUSINESS  some,talk,american,banker,said,polit,pressur,a..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_classifier.train_data['BUSINESS'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:40.188493Z",
     "start_time": "2020-05-02T12:44:40.180215Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6768 entries, 0 to 2102\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   index              6768 non-null   int64 \n",
      " 1   category           6768 non-null   object\n",
      " 2   short_description  6768 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 211.5+ KB\n"
     ]
    }
   ],
   "source": [
    "sample_classifier.train_data['BUSINESS'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "The test data are also grouped by their categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:40.199428Z",
     "start_time": "2020-05-02T12:44:40.190485Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BUSINESS':       index  category                                  short_description\n",
       " 3654  18365  BUSINESS  we,live,corpor,environ,often,place,great,impor...\n",
       " 3655  18370  BUSINESS  thi,year,term,reput,everywher,it,longer,primar...\n",
       " 3656  18373  BUSINESS                   here,challeng,take,love,put,work\n",
       " 3657  18374  BUSINESS  we,got,test,new,game,suppos,better,teach,strat...\n",
       " 3658  18378  BUSINESS  madison,nicol,robinson,design,best,known,creat...\n",
       " ...     ...       ...                                                ...\n",
       " 4563  22876  BUSINESS  in,addit,stronger,rule,need,make,sure,system,n...\n",
       " 4564  22884  BUSINESS  privat,prison,corpor,geo,group,grew,1980,state...\n",
       " 4565  22892  BUSINESS  low,price,make,oil,tanker,heist,africa,western...\n",
       " 4566  22910  BUSINESS  simultan,without,know,i,learn,mani,valuabl,les...\n",
       " 4567  22914  BUSINESS  the,bank,pledg,april,stop,financ,compani,sell,...\n",
       " \n",
       " [914 rows x 3 columns],\n",
       " 'TRAVEL':       index category                                  short_description\n",
       " 6768  18251   TRAVEL  how,far,would,airlin,go,collect,buck,if,noth,w...\n",
       " 6769  18252   TRAVEL  when,consid,herculean,effort,get,away,unpredic...\n",
       " 6770  18257   TRAVEL  pervert,ladi,germ,love,land,educ,sex,film,scre...\n",
       " 6771  18258   TRAVEL  romanc,aliv,well,line,southern,charm,make,ever...\n",
       " 6772  18259   TRAVEL  love,thrill,downhil,sport,lose,woolli,sock,the...\n",
       " ...     ...      ...                                                ...\n",
       " 8456  22894   TRAVEL          what,airlin,name,some,truli,monik,pli,sky\n",
       " 8457  22897   TRAVEL  everyon,love,littl,extra,legroom,fli,legroom,e...\n",
       " 8458  22898   TRAVEL  northern,europ,scandinavia,baltic,sea,lend,cru...\n",
       " 8459  22922   TRAVEL  travelzoo,global,internet,media,compani,celebr...\n",
       " 8460  22923   TRAVEL  we,want,save,bought,intent,creat,place,could,r...\n",
       " \n",
       " [1693 rows x 3 columns]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_classifier.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:40.211315Z",
     "start_time": "2020-05-02T12:44:40.200976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>18365</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>we,live,corpor,environ,often,place,great,impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>18370</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>thi,year,term,reput,everywher,it,longer,primar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>18373</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>here,challeng,take,love,put,work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>18374</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>we,got,test,new,game,suppos,better,teach,strat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>18378</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>madison,nicol,robinson,design,best,known,creat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  category                                  short_description\n",
       "3654  18365  BUSINESS  we,live,corpor,environ,often,place,great,impor...\n",
       "3655  18370  BUSINESS  thi,year,term,reput,everywher,it,longer,primar...\n",
       "3656  18373  BUSINESS                   here,challeng,take,love,put,work\n",
       "3657  18374  BUSINESS  we,got,test,new,game,suppos,better,teach,strat...\n",
       "3658  18378  BUSINESS  madison,nicol,robinson,design,best,known,creat..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_classifier.test_data['BUSINESS'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:44:40.222408Z",
     "start_time": "2020-05-02T12:44:40.213994Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 914 entries, 3654 to 4567\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   index              914 non-null    int64 \n",
      " 1   category           914 non-null    object\n",
      " 2   short_description  914 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 21.6+ KB\n"
     ]
    }
   ],
   "source": [
    "sample_classifier.test_data['BUSINESS'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, there are 4568 pieces of news in the BUSINESS category, but after oversampling in the sample_classifier, there are 6939 pieces of news in the BUSINESS category in the training data (Oversampling only affects the training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Phase 1 results\n",
    "\n",
    "The phase 1 and 2 results are the _Recall_ and _Precision_ values for each category and the _accuracy_ of the whole prediction process.\n",
    "\n",
    "These values are evaluated from the following formulas:\n",
    "\n",
    "$$Recall = \\frac{Correct\\ Category\\ Predictions}{Total\\ Category\\ News\\ Count}$$\n",
    "\n",
    "$$Precision = \\frac{Correct\\ Category\\ Predictions}{Total\\ Category\\ Predictions}$$\n",
    "\n",
    "$$Accuracy = \\frac{Correct\\ Category\\ Predictions}{Total\\ Data\\ Count}$$\n",
    "\n",
    "Phase 1 considers only the 'BUSINESS' and 'TRAVEL' categories.\n",
    "\n",
    "### What happens if only precision is considered?\n",
    "The precision value shows the number of true positives divided by the sum of true positives and false positives. In this manner, if a model predicts only one true positive, it's precision goes to 1.0! But this measurement completely ignores the false negatives the model predicted. For example consider a model which has to detect diseases. In a dataset with a 1000 pieces of data from which 10 are positives, the model only predicts 2 true positives. It's precision will be 1.0 as there are no false positives, but this won't make it a good model as it has had 8 false negatives while predicting. The recall for such a prediction would be 20% which is not good at all. So with combining different measures such as recall, precision and accuracy, this problem will be solved but considering precision only won't result in a good model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:45:23.052430Z",
     "start_time": "2020-05-02T12:44:40.226386Z"
    }
   },
   "outputs": [],
   "source": [
    "def phase_1():\n",
    "    stemmized_classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method='stemming', \n",
    "        desired_categories=['TRAVEL', 'BUSINESS'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80,\n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=True\n",
    "    )\n",
    "    stemmization_test_results = stemmized_classifier.check_with_test_data()\n",
    "    \n",
    "    lemmatized_classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method='lemmatization', \n",
    "        desired_categories=['TRAVEL', 'BUSINESS'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80,\n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=True\n",
    "    )\n",
    "    lemmatization_test_results = lemmatized_classifier.check_with_test_data()\n",
    "    return {\n",
    "        'lemmatization': lemmatization_test_results,\n",
    "        'stemming': stemmization_test_results\n",
    "    }\n",
    "    \n",
    "p1_results = phase_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:45:23.058299Z",
     "start_time": "2020-05-02T12:45:23.054627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| Category | Normalization Method | Oversampling |       Recall       |     Precision      |      Accuracy      |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| BUSINESS |    lemmatization     |     True     | 0.8599562363238512 |      0.81875       | 0.8841580360567702 |\n",
      "|  TRAVEL  |    lemmatization     |     True     | 0.8972238629651507 | 0.9222829386763813 | 0.8841580360567702 |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| Category | Normalization Method | Oversampling |       Recall       |     Precision      |      Accuracy      |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| BUSINESS |       stemming       |     True     | 0.8577680525164114 | 0.814968814968815  | 0.8818565400843882 |\n",
      "|  TRAVEL  |       stemming       |     True     | 0.8948611931482575 | 0.9209726443768997 | 0.8818565400843882 |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "for norm_method, results in p1_results.items():\n",
    "    print_results_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Phase 1 results without oversampling\n",
    "This part demonstrates the effect of oversampling on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:46:02.803290Z",
     "start_time": "2020-05-02T12:45:23.060519Z"
    }
   },
   "outputs": [],
   "source": [
    "def phase_1_without_oversampling():\n",
    "    stemmized_classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method='stemming', \n",
    "        desired_categories=['TRAVEL', 'BUSINESS'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80,\n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=False\n",
    "    )\n",
    "    stemmization_test_results = stemmized_classifier.check_with_test_data()\n",
    "    \n",
    "    lemmatized_classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method='lemmatization', \n",
    "        desired_categories=['TRAVEL', 'BUSINESS'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80, \n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=False\n",
    "    )\n",
    "    lemmatization_test_results = lemmatized_classifier.check_with_test_data()\n",
    "    \n",
    "    return {\n",
    "        'lemmatization': lemmatization_test_results,\n",
    "        'stemming': stemmization_test_results\n",
    "    }\n",
    "\n",
    "p1_results_without_oversampling = phase_1_without_oversampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:46:02.813192Z",
     "start_time": "2020-05-02T12:46:02.807419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| Category | Normalization Method | Oversampling |       Recall       |     Precision      |      Accuracy      |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| BUSINESS |    lemmatization     |    False     | 0.8916849015317286 | 0.7560296846011132 | 0.8611430763329497 |\n",
      "|  TRAVEL  |    lemmatization     |    False     | 0.8446544595392794 | 0.935251798561151  | 0.8611430763329497 |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| Category | Normalization Method | Oversampling |       Recall       |     Precision      |      Accuracy      |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "| BUSINESS |       stemming       |    False     | 0.8894967177242888 | 0.777246653919694  | 0.8718833908707326 |\n",
      "|  TRAVEL  |       stemming       |    False     | 0.8623744831659775 | 0.9352978859705318 | 0.8718833908707326 |\n",
      "+----------+----------------------+--------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "for norm_method, results in p1_results_without_oversampling.items():\n",
    "    print_results_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Phase 2 results\n",
    "This phase calculates the recall, precision and accuracy values for 'BUSINESS', 'TRAVEL' and 'STYLE & BEAUTY' categories.\n",
    "The confusion matrix is also printed for this prediction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:46:42.712901Z",
     "start_time": "2020-05-02T12:46:02.816283Z"
    }
   },
   "outputs": [],
   "source": [
    "def phase_2():\n",
    "    stemmized_classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method='stemming', \n",
    "        desired_categories=['TRAVEL', 'BUSINESS', 'STYLE & BEAUTY'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80,\n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=True\n",
    "    )\n",
    "    stemmization_test_results = stemmized_classifier.check_with_test_data()\n",
    "    \n",
    "    lemmatized_classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method='lemmatization', \n",
    "        desired_categories=['TRAVEL', 'BUSINESS', 'STYLE & BEAUTY'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80,\n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=True\n",
    "    )\n",
    "    lemmatization_test_results = lemmatized_classifier.check_with_test_data()\n",
    "    \n",
    "    return {\n",
    "        'lemmatization': lemmatization_test_results,\n",
    "        'stemming': stemmization_test_results\n",
    "    }\n",
    "    \n",
    "p2_results = phase_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:46:42.718547Z",
     "start_time": "2020-05-02T12:46:42.714905Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+--------------+--------------------+--------------------+-------------------+\n",
      "|    Category    | Normalization Method | Oversampling |       Recall       |     Precision      |      Accuracy     |\n",
      "+----------------+----------------------+--------------+--------------------+--------------------+-------------------+\n",
      "|    BUSINESS    |    lemmatization     |     True     | 0.824945295404814  | 0.7384916748285995 | 0.845923537540304 |\n",
      "|     TRAVEL     |    lemmatization     |     True     | 0.8417011222681631 | 0.8631132646880678 | 0.845923537540304 |\n",
      "| STYLE & BEAUTY |    lemmatization     |     True     | 0.8610951008645533 | 0.8946107784431138 | 0.845923537540304 |\n",
      "+----------------+----------------------+--------------+--------------------+--------------------+-------------------+\n",
      "+----------------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|    Category    | Normalization Method | Oversampling |       Recall       |     Precision      |      Accuracy      |\n",
      "+----------------+----------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|    BUSINESS    |       stemming       |     True     | 0.8238512035010941 | 0.744807121661721  | 0.8473053892215568 |\n",
      "|     TRAVEL     |       stemming       |     True     | 0.8446544595392794 | 0.8624849215922799 | 0.8473053892215568 |\n",
      "| STYLE & BEAUTY |       stemming       |     True     | 0.8622478386167147 | 0.8942020322773461 | 0.8473053892215568 |\n",
      "+----------------+----------------------+--------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "for norm_method, results in p2_results.items():\n",
    "    print_results_table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:46:42.731320Z",
     "start_time": "2020-05-02T12:46:42.720185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Method: lemmatization\n",
      "               |         S      |\n",
      "               |         T      |\n",
      "               |         Y      |\n",
      "               |         L      |\n",
      "               |         E      |\n",
      "               |                |\n",
      "               |    B    &      |\n",
      "               |    U           |\n",
      "               |    S    B    T |\n",
      "               |    I    E    R |\n",
      "               |    N    A    A |\n",
      "               |    E    U    V |\n",
      "               |    S    T    E |\n",
      "               |    S    Y    L |\n",
      "---------------+----------------+\n",
      "      BUSINESS | <754>  66   94 |\n",
      "STYLE & BEAUTY |  109<1494> 132 |\n",
      "        TRAVEL |  158  110<1425>|\n",
      "---------------+----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Normalization Method: stemming\n",
      "               |         S      |\n",
      "               |         T      |\n",
      "               |         Y      |\n",
      "               |         L      |\n",
      "               |         E      |\n",
      "               |                |\n",
      "               |    B    &      |\n",
      "               |    U           |\n",
      "               |    S    B    T |\n",
      "               |    I    E    R |\n",
      "               |    N    A    A |\n",
      "               |    E    U    V |\n",
      "               |    S    T    E |\n",
      "               |    S    Y    L |\n",
      "---------------+----------------+\n",
      "      BUSINESS | <753>  63   98 |\n",
      "STYLE & BEAUTY |  109<1496> 130 |\n",
      "        TRAVEL |  149  114<1430>|\n",
      "---------------+----------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for norm_method, data in p2_results.items():\n",
    "    print('Normalization Method:', data['normalization_method'])\n",
    "    print(data['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final phase of the assignment\n",
    "This phase tries to predict the categories of some uncalssified data and writes the results to the given file (in this assignment, the output file is 'output.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T12:46:56.877875Z",
     "start_time": "2020-05-02T12:46:42.733379Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_phase(normalization_method):\n",
    "    classifier = Classifier(\n",
    "        dataset=dataset,\n",
    "        normalization_method=normalization_method, \n",
    "        desired_categories=['TRAVEL', 'BUSINESS', 'STYLE & BEAUTY'], \n",
    "        test_data_filename=TEST_FILENAME, \n",
    "        training_data_percentage=80,\n",
    "        output_filename=OUTPUT_FILENAME,\n",
    "        oversample=True\n",
    "    )\n",
    "    results = classifier.predict()\n",
    "    \n",
    "final_phase('stemming')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
